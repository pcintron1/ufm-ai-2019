{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una red neuronal compuesta por dos partes:\n",
    "    * Un _encoder_ network que comprime input data de alta dimension a una representacion vectorial de baja dimension\n",
    "    * Un _decoder_ network que decomprime una representacion vectorial dada de regreso al dominio original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama de Autoencoder](../assets/autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La red esta entrenada para encontrar los pesos para el encoder y decoder que minimizan el loss entre\n",
    "    * el input original\n",
    "    * la reconstruccion del input\n",
    "* El vector de representacion es una compresion de la imagen original a un espacio latente con baja dimensionalidad.\n",
    "* La idea es que al escoger _cualquier_ punto en el espacio latente, deberiamos poder generar imagenes nuevas al pasar este punto a traves del decoder.\n",
    "    * Porque el decoder aprendio el mapa: puntos en espacio latente -> imagenes viables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a empezar construyendo un autoencoder simple para comprimir el dataset de MNIST. Con autoencoders, pasamos input data a traves del encoder que crea la representacion comprimida del input. Luego, esta representacion pasa a traves del decoder para reconstruir la data de input. Generalmente el encoder y decoder se construyen usando NNs, luego se entrenan en data de ejemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representacion comprimida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una representacion comprimida puede ser buena para guardar y compartir cualquier tipo de data en una forma que sea mas eficiente que guardar la data cruda. En la practica, la representacion comprimida contiene informacion clave sobre la imagen de input y la podemos usar para reconstruir, denoising y otras transformaciones.\n",
    "\n",
    "En este notebook vamos a construir una simple NN para el encoder y decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# convertira data a torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# cargar los datasets de training y test\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear los dataloaders para training y test\n",
    "# numero de subprocesses para usar para cargar la data\n",
    "num_workers = 0\n",
    "# numero de muestras por batch para cargar\n",
    "batch_size = 20\n",
    "\n",
    "# data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder lineal\n",
    "\n",
    "Vamos a entrenar un autoencoder con estas imagenes aplanandolas a vectores de largo 784. Las imagenes de este dataset ya estan normalizadas para que sus valores esten entre 1 y 0. El encoder y decoder deberian estar hechos de **una capa lineal**. Las unidades que conectan el encoder y decoder van a ser la _representacion comprimida_.\n",
    "\n",
    "Como las imagenes estan normalizadas, necesitamos usar una activacion **sigmoid** en la capa de output para obtener valores que se encuentren en el mismo rango que el input.\n",
    "\n",
    "**TODO: Construir el autoencoder en la celda de abajo**\n",
    "> Las imagenes de input deben ser aplanadas a vectores de 784. Los targets son los mismos que los inputs. El encoder y el decoder van a estar hechos de dos capas lineales cada uno. La profundidad de las dimensiones deben cambiar de la siguiente forma: 784 inputs -> **encoding_dims** -> 784 outputs. Todas las capas deben tener activaciones ReLu aplicadas excepto por la capa final que debe ser un sigmoid\n",
    "\n",
    "**La representacion comprimida debe ser un vector con dimension `encoding_dim=32`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definir arquitectura\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        ## encoder ##\n",
    "        \n",
    "        ## decoder ##\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # definir feedforward\n",
    "        # aplicar sigmoid al output layer\n",
    "        \n",
    "        return x\n",
    "\n",
    "# inicializar NN\n",
    "encoding_dim = 32\n",
    "model = Autoencoder(encoding_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resto es codigo para entrenamiento, deberia resultarles familiar. En este caso no nos interesa mucho la validacion, asi que solo vamos a monitorear el training loss y el test loss.\n",
    "\n",
    "Tampoco nos preocupamos por los labels en este caso. Como estamos comparando valores de pixeles en las imagenes de input y output, lo mejor es usar una los que sea util para tareas de regresion. La regresion se utiliza para comparar _cantidades_ en vez de valores probabilisticos. En este caso vamos a usar `MSELoss`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-41aa826489a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# specify loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# specify loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# especificar loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# especificar optimizador\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numero de epochs para entrenar el modelo\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ######################\n",
    "    # entrenar el modelo #\n",
    "    ######################\n",
    "    for data in train_loader:\n",
    "        # _ para los labels (como no nos interesan)\n",
    "        images, _ = data\n",
    "        # aplanar imagenes\n",
    "        images = images.view(images.size(0), -1)\n",
    "        # limpiar las gradientes de todas las variables optimizadas\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: calcular valores predecidos\n",
    "        outputs = model(images)\n",
    "        # calcular el loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: calcular gradiente del los con respecto al modelo\n",
    "        loss.backward()\n",
    "        # realizar un unico paso de optimizacion (actualizar parametros)\n",
    "        optimizer.step()\n",
    "        # actualizar training loss acumulado\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print estadisticas de entrenamiento\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisar los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El codigo de abajo son plots de las imagenes de entrenamiento con sus reconstrucciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtener un batch del test set\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images_flatten = images.view(images.size(0), -1)\n",
    "# sample outputs\n",
    "output = model(images_flatten)\n",
    "# preparar imagenes para display\n",
    "images = images.numpy()\n",
    "\n",
    "# output es redimensionado a un batch de imagenes\n",
    "output = output.view(batch_size, 1, 28, 28)\n",
    "# usar detach cuando es un output que requires_grad=False\n",
    "output = output.detach().numpy()\n",
    "\n",
    "# plot las primeras 10 input images y luego reconstruir las imagenes\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# inputs arriba, reconstrucciones abajo\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos tratando con imagenes, por lo que (usualmente) obtendriamos mejor performance usando convolution layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo con el dataset de MNIST, vamos a mejorar el performance del autoencoder usando convolutional layers. Vamos a construir un autoencoder para comprimir el dataset de MNIST.\n",
    "> La porcion del encoder estara compuesta de convolutional y pooling layers y el decoder va a estar hecho de **transpose convolutional layers** que aprenden a \"upsample\" la representacion comprimida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ad17e6b7cd06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 20\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder**\n",
    "\n",
    "La parte de la red para el encoder va a ser una tipica piramide de convolutions. Cada convolutional layer va a ser seguida por un layer de max-pooling para reducir las dimensiones de los layers.\n",
    "\n",
    "**Decoder**\n",
    "\n",
    "El decoder si es algo nuevo. El decoder necesita convertir una representacion estrecha a una imagen ancha reconstruida. Por ejemplo, la representacion puede ser un max-pool layer de 7x7x4. Este es el outpu del encoder, pero tambien el input para el encoder. Queremos obtener una imagen de 28x28x1 de este decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![autoencoder architecture](../assets/conv_enc_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra ultima layer del encoder tiene un tamanio de 7x7x4 = 196. Las imagenes originales tienen tamanio 28x28 = 784, asi que el encoded vector es 25% el tamanio de la imagen original. Estos son solamente tamanios sugeridos para cada layer. Intenten cambiar las depths y sizes y agregar layers adicionales para hacer esta representacion hasta mas pequenia. Nuestra meta aqui es encontrar una representacion pequenia del input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose convolutions, Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este decoder usa **transposed convolutional** layers para incrementar el ancho y altura de las input layers. Funcionan casi exactamente igual que las convolutional layers, pero en reversa. Un stride en el input layer resulta en un stride mas largo en el transposed convolution layer. Por ejemplo, si tenemos un kernel de 3x3, un patch de 3x3 en el input layer va a ser reducido a una unidad en el convolutional layer. Asimismo, una unidad en el input layer se va a expandir a un patch de 3x3 en un transposed convolution layer. PyTorch nos provee con una forma facil de crear los layers, [`nn.ConvTranspose2d`](https://pytorch.org/docs/stable/nn.html#convtranspose2d).\n",
    "\n",
    "Es importante notar que los transpose convolution layers pueden introducir artefactos en las imagenes finales, como patrones cuadriculados. Esto se debe al overlap en los kernels los cuales pueden ser evitados configurando el tamanio del stride y el kernel para que sean del mismo tamanio. En este [articulo de Distill](http://distill.pub/2016/deconv-checkerboard/) de Augustus Odena, _et. al_, los autores demuestran como estos artefactos pueden ser evitados.\n",
    "\n",
    "**TODO: Desarrollar la red**\n",
    "> Construir el encoder compuesto de una serie de convolutional y pooling layers. Cuando construyan el decoder, recuerden que los transpose convolutional layers pueden upsample un input por un factor de 2 usando un stride y un kernel_size de 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# definir la arquitectura\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        \n",
    "        \n",
    "        ## decoder layers ##\n",
    "        ## un kernel de 2 y un stride de 2 van a incrementar las dims por 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        \n",
    "        ## decode ##\n",
    "        ## aplicar ReLu a todas las hidden layers excepto el output layer\n",
    "        ## aplicar sigmoid al output layer\n",
    "        \n",
    "                \n",
    "        return x\n",
    "\n",
    "# inicializar el NN\n",
    "model = ConvAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for data in train_loader:\n",
    "        images, _ = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "output = model(images)\n",
    "images = images.numpy()\n",
    "\n",
    "output = output.view(batch_size, 1, 28, 28)\n",
    "output = output.detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
