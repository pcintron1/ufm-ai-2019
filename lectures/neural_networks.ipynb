{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El uso de funciones de activacion no lineares como la diferencia clave entre modelos lineales\n",
    "* Los diferentes tipos de funciones de activacion\n",
    "* El modulo `nn` de PyTorch que contiene los bloques para construir NNs\n",
    "* Resolver un problema simple de un _fit_ lineal con una NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronas artificiales\n",
    "\n",
    "* Neural networks: entidades matematicas capaces de representar funciones complicadas a traves de una composicion de funciones mas simples.\n",
    "* Originalmente inspiradas por la forma en la que funciona nuestro cerebro.\n",
    "* El bloque de construccion basico es una neurona:\n",
    "    * Esencialmente una transformacion linear del input (e.g. multiplicacion del input por un numero, el _weight_, y la suma de una constante, el _bias_.\n",
    "    * Seguido por la aplicacion de una funcion no lineal (referida como la funcion de activacion)\n",
    "    * $o = f(w x + b)$\n",
    "    * x es nuestro input, w el _weight_ y b el _bias_. $f$ es la funcion de activacion.\n",
    "    * x puede ser un escalar o un vector de valores, w puede ser un escalar o una matriz, mientras que b es un escalar o un vector.\n",
    "* La expresion $o = f(w x + b)$ es una capa de neuronas, ya que representa varias neuronas a traves de los _weights_ y _bias_ multidimensionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_1 = f(w_0 x_0 + b_0)$\n",
    "\n",
    "$x_2 = f(w_1 x_1 + b_1)$\n",
    "\n",
    "$...$\n",
    "\n",
    "$y = f(w_n x_n + b_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **dibujos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de activacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nuestro modelo anterior ya tenia una operacion lineal. Eso era el modelo entero.\n",
    "* El rol de la funcion de activacion es concentrar los _outputs_ de la operacion lineal precedente a un rango dado.\n",
    "* Si queremos asignar un _score_ al output del modelo necesitamos limitar el rango de numeros posibles para ese _score_\n",
    "    * `float32`\n",
    "    * $\\sum wx + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Que opciones tenemos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una opcion seria ponerle un limite a los valores del _output_.\n",
    "    * Cualquier cosa debajo de cero seria cero\n",
    "    * cualquier cosa arriba de 10 seria 10\n",
    "    * `torch.nn.Hardtanh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.tanh(-2.2) # camion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.tanh(0.1) # oso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.tanh(2.5) # perro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Funciones de activacion](../assets/activaciones.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hay muchas funciones de activacion.\n",
    "* Por definicion, las funciones de activacion:\n",
    "    * Son no lineales. Aplicaciones repetidas de $wx+b$ sin una funcion de activacion resultan en una polinomial. La no linealidad permite a la red aproximar funciones mas complejas.\n",
    "    * Son diferenciables, para poder calcular las gradientes a traves de ellas. Discontinuidades de punto como en `Hatdtanh` o `ReLU` son validas.\n",
    "* Sin esto, las redes caen a ser polinomiales complicadas o dificiles de entrenar.\n",
    "* Adicionalmente, las funciones:\n",
    "    * Tienen al menos un rango sensible, donde cambios no triviales en el input resultan en cambio no trivial correspondiente en el output\n",
    "    * Tienen al menos un rango no sensible (o saturado), donde cambios al input resultan en poco o ningun cambio en el output.\n",
    "* Por utlimo, las fuciones de activacion tienen al menos una de estas:\n",
    "    * Un limite inferior que se aproxima (o se encuentra) mientras el input tiende a negativo infinito.\n",
    "    * Un limite superior similar pero inverso para positivo infinito.\n",
    "* Dado lo que sabemos de como funciona back-propagation\n",
    "    * Sabemos que los errores se van a propagar hacia atras a traves de la activacion de manera mas efectiva cuando los inputs se encuentran dentro del rango de respuesta.\n",
    "    * Por otro lado, los errores no van a afectar a las neuornas para cuales el _input_ esta saturado debido a que la gradiente estara cercana a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En conclusion\n",
    "\n",
    "* En una red hecha de unidades lineales + activaciones, cuando recibe diferentes _inputs_:\n",
    "    * diferentes unidades van a responder en diferentes rangos para los mismos inputs\n",
    "    * los errores asociados a esos inputs van a afectar a las neuronas operancio en el rango sensible, dejando a las otras unidades mas o menos igual en el proceso de aprendizaje. \n",
    "* Juntar muchas operaciones lineales + unidades de activacion en paralelo y apilandolas una sobre otra nos provee un objeto matematico capaz de aproximar funciones complicadas. \n",
    "* Diferentes combinaciones de unidades van a responder a inputs en diferentes rangos\n",
    "    * Esos parametros son relativamente faciles de optimizar a traves de SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje para Neural Networks\n",
    "\n",
    "* Una red entrenada exitosamente, a traves de los valores de sus _weights_ y _biases_, va a capturar la estructura inherente de la data.\n",
    "* Esta estrcutura se captura en la forma de representaciones numericas significativas que funcionan de forma correcta para data que no ha visto anteriormente.\n",
    "* Los NNs nos proveen la habilidad de aproximar fenomenos altamente no lineales sin tener que tener un modelo explicito.\n",
    "    * En vez empezamos con un model generico, no entrenado y lo especializamos a una tarea al proveerle:\n",
    "        * un set de inputs\n",
    "        * un set de outputs\n",
    "        * una _loss function_ desde la cual puede realizar el back-propagation\n",
    "    * Especializar un modelo generico a una tarea especifica, usando ejemplos es a lo que nos referimos por _aprendizaje_\n",
    "    * El modelo no se construyo con esa tarea especifica en mente. No codificamos en el modelo reglas que describen como funciona la tarea.\n",
    "    \n",
    "    \n",
    "* Para nuestro modelo del termometro asumimos que las temperaturas se median de forma lineal.\n",
    "    * En esa suposicion implicitamente codificamos reglas para nuestra tarea: especificamos la forma de nuestra funcion input/output.\n",
    "    * No hubiesemos podido aproximar nada mas que no fueran puntos al rededor de una linea.\n",
    "* Mientras la dimensionalidad de un problema crece y las relaciones entre inputs/outputs se complican, asumir la forma de la funcion probablemente no va a funcionar.\n",
    "* De cierta forma estamos renunciando la interpretabilidad por la posibilidad de solucionar problemas mas complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El modulo `nn` de PyTorch\n",
    "\n",
    "* Vamos a reemplazar nuestro modelo lineal por un NN.\n",
    "* PyTorch tiene un submodulo entero dedicado a NNs llamado `torch.nn`\n",
    "* Contiene los bloques necesarios para construir todo tipo de arquitecturas de NNs.\n",
    "* Esos bloques se llaman _modules_ en PyTorch (en otros frameworks se llaman _layers_)\n",
    "* Un modulo de PyTorch es una clase de Python que deriva de la clase base `nn.Module`.\n",
    "    * Un modulo puede tener una o mas instancias de `Parameter` como atributos\n",
    "    * Estos son tensores cuyos valores son optimizados durante el proceso de entrenamiento ($w$ y $b$)\n",
    "    * Un modulo tambien puede tener uno o mas submodulos (subclases de `nn.Module` como atributos) y va a poder llevar un registro de sus `Parameter`s de igual forma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dibujo graficas computacionales separadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "linear_model = nn.Linear(1, 1)\n",
    "linear_model(val_t_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las subclases de `nn.Module` tienen un metodo `call` definido. Esto permite crear una instancia de `nn.Linear` y llamarla como si fuera una funcion.\n",
    "\n",
    "Llamar una instancia de `nn.Module` con un conjunto de argumetnos termina llamando un metodo llamado `forward` con esos mismos argumentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacion de `Module.call`\n",
    "\n",
    "(simplificado para claridad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, *input, **kwargs):\n",
    "    for hook in self._forward_pre_hooks.values():\n",
    "        hook(self, input)\n",
    "        \n",
    "    result = self.forward(*input, **kwargs)\n",
    "    \n",
    "    for hook in self._forward_hooks.values():\n",
    "        hook_result = hook(self, input, result)\n",
    "        # ...\n",
    "        \n",
    "    for hook in self._backward_hooks.values():\n",
    "        # ...\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De regreso al modelo lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "linear_model = nn.Linear(1, 1)\n",
    "linear_model(val_t_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Linear` acepta tres argumentos:\n",
    "* el numero de input features: size del input = 1\n",
    "* numero de output features: size del outpu = 1\n",
    "* si incluye un bias o no (por default es `True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nuestro modelo toma un input y produce un output\n",
    "* `nn.Module` y sus subclases estan diseniados para hacer eso sobre multiples muestras al mismo tiempo\n",
    "* Para acomodar multiples muestras los modulos esperan que la dimension 0 del input sea el numero de muestras en un _batch_\n",
    "* Cualquier module en `nn` esta hecho para producir outputs para un _batch_ de multiples inputs al mismo tiempo.\n",
    "* B x Nin\n",
    "    * B es el tamanio del _batch_\n",
    "    * Nin el numero de input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(10, 1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un dataset de imagenes:\n",
    "* BxCxHxW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0] # Temperatura en grados celsios\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] # Unidades desconocidas\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # Agregamos una dimension para tener B x N_inputs\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # Agregamos una dimension para tener B x N_inputs\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "params_old = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate_old = 1e-1\n",
    "optimizer_old = optim.Adam([params], lr=learning_rate)\n",
    "\n",
    "\n",
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(\n",
    "    linear_model.parameters(), # reemplazamos [params] con este metodo \n",
    "    lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(linear_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, n_epochs, optimizer, loss_fn, train_x, val_x, train_y, val_y):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_x) # ya no tenemos que pasar los params\n",
    "        train_loss = loss_fn(train_t_p, train_y)\n",
    "        \n",
    "        with torch.no_grad(): # todos los args requires_grad=False\n",
    "            val_t_p = model(val_x)\n",
    "            val_loss = loss_fn(val_t_p, val_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss}, Validation loss {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=1e-2)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=3000,\n",
    "    optimizer=optimizer,\n",
    "    model=linear_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "print()\n",
    "print(linear_model.weight)\n",
    "print(linear_model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalmente un Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ultimo paso: reemplazar nuestro modelo lineal\n",
    "* No va a ser mejor\n",
    "* Lo unico que vamos a cambiar va a ser el modelo\n",
    "* Un simple NN:\n",
    "    * Una capa lineal\n",
    "    * Activacion\n",
    "    * \"hidden layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = nn.Sequential(\n",
    "                nn.Linear(1, 13), # El 13 es arbitrario\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(13, 1) # Este 13 debe hacer match con el primero\n",
    "            )\n",
    "\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El resultado final es un modelo que toma los inputs esperados por el primer modulo (_layer_)\n",
    "* Pasa los outputs intermedios al resto de los modulos\n",
    "* Produce un output retornado por el ultimo modulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[param.size() for param in seq_model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estos son los parametros que el optimizador va a recibir\n",
    "* Al llamar `backward()` todos los parametros se van a llenar con su `grad`\n",
    "* El optimizador va a actualizar el valor de `grad` durante `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "named_seq_model = nn.Sequential(OrderedDict([\n",
    "        ('hidden_linear', nn.Linear(1, 8)),\n",
    "        ('hidden_activation', nn.Tanh()),\n",
    "        ('output_linear', nn.Linear(8, 1))\n",
    "]))\n",
    "\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in named_seq_model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_seq_model.output_linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Util para inspeccionar parametros o sus gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5000,\n",
    "    optimizer=optimizer,\n",
    "    model=seq_model,\n",
    "    loss_fn=nn.MSELoss(), # Ya no estamos usando nuestra loss function hecha a mano\n",
    "    train_x = train_t_un,\n",
    "    val_x = val_t_un,\n",
    "    train_y = train_t_c,\n",
    "    val_y = val_t_c)\n",
    "\n",
    "print('output', seq_model(val_t_un))\n",
    "print('answer', val_t_c)\n",
    "print('hidden', seq_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien podemos evaluar el modelo en toda la data y ver que tan diferente es de una linea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t_range = torch.arange(20., 90.).unsqueeze(1)\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Fahrenheit\")\n",
    "plt.ylabel(\"Celsius\")\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "plt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-')\n",
    "plt.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing nn.Module\n",
    "\n",
    "* sublcassing `nn.Module` nos da mucha mas flexibilidad.\n",
    "* La interface especifica que como minimo debemos definir un metodo `forward` para la subclase\n",
    "    * `forward` toma el input al model y regresa el output\n",
    "* Si usamos las operaciones de `torch`, `autograd` se encarga de hacer el `backward` pass de forma automatica\n",
    "\n",
    "* Normalmente vamos a definir los submodulos que usamos en el metodo `forward` en el constructor\n",
    "    * Esto permite que sean llamados en `forward` y que puedan mantener sus parametros a durante la existencia de nuestro modulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubclassModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(1, 13)\n",
    "        self.hidden_activation = nn.Tanh()\n",
    "        self.output_linear = nn.Linear(13, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden_t = self.hidden_linear(input)\n",
    "        activated_t = self.hidden_activation(hidden_t)\n",
    "        #activated_t = self.hidden_activation(hidden_t) if random.random() > 0.5 else hidden_t\n",
    "        output_t = self.output_linear(activated_t)\n",
    "\n",
    "        return output_t\n",
    "\n",
    "    \n",
    "subclass_model = SubclassModel()\n",
    "subclass_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nos permite manipular los outputs de forma directa  y transformarlo en un tensor BxN\n",
    "* Dejamos la dimension de batch como -1 ya que no sabemos cuantos inputs van a venir por batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Asignar una instancia de `nn.Module` a un atributo en un `nn.Module` registra el modulo como un submodulo.\n",
    "* Permite a `Net` acceso a los `parameters` de sus submodulos sin necesidad de hacerlo manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numel_list = [p.numel() for p in subclass_model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lo que paso**\n",
    "\n",
    "* `parameters()` investiga todos los submodulos asignados como atributos del constructor y llama `parameters` de forma recursiva.\n",
    "* Al accesar su atributo `grad`, el cual va a ser llenado por el `autograd`, el optimizador va a saber como cambiar los parametros para minimizar el _loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_str, model in [('seq', seq_model), ('named_seq', named_seq_model), ('subclass', subclass_model)]:\n",
    "    print(type_str)\n",
    "    for name_str, param in model.named_parameters():\n",
    "        print(\"{:21} {:19} {}\".format(name_str, str(param.shape), param.numel()))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubclassFunctionalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_linear = nn.Linear(1, 14)\n",
    "        self.output_linear = nn.Linear(14, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden_t = self.hidden_linear(input)\n",
    "        activated_t = torch.tanh(hidden_t)\n",
    "        output_t = self.output_linear(activated_t)\n",
    "\n",
    "        return output_t\n",
    "\n",
    "\n",
    "func_model = SubclassFunctionalModel()\n",
    "func_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Experimenten con el numero de neuronas en el modelo al igual que el learning rate.\n",
    "    * Que cambios resultan en un output mas lineal del modelo?\n",
    "    * Pueden hacer que el modelo haga un overfit obvio de la data?\n",
    "    \n",
    "* Cargen la [data de vinos blancos](https://archive.ics.uci.edu/ml/datasets/wine+quality) y creen un modelo con el numero apropiado de inputs\n",
    "    * Cuanto tarda en entrenar comparado al dataset que hemos estado usando?\n",
    "    * Pueden explicar que factores contribuyen a los tiempos de entrenamiento?\n",
    "    * Pueden hacer que el _loss_ disminuya?\n",
    "    * Intenten graficar la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
