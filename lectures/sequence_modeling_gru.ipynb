{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Secuencias\n",
    "### Intermedios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las tareas de prediccion de secuencias requiren que etiquetemos cada item en una secuencia\n",
    "* Estas tareas son comunes en NLP:\n",
    "    * _language modeling_: predecir la siguiente palabra dada una secuencia de palabras en cada paso.\n",
    "    * _named entity recognition_: predecir si cada palabra es parte de una entidad nombrada como, persona, localizacion, producto o organizacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sequence prediction tasks](../assets/seq-prediction-tasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dos Ejemplos de tareas de prediccion de secuencias**\n",
    "* (a) _language modeling_ donde la tarea es predecir la siguiente palabra en una secuencia.\n",
    "* (b) _named entity recognition_ donde la tarea es predecir los limites de de una entidad y su tipo de entidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Las debilidades del Elman RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aunque el Elman RNN es adecuado para modelar secuencias, tiene dos problemas que lo hacen poco adecuado para muchas tareas:\n",
    "    1. La inabilidad de retener informacion para predicciones a largo plazo.\n",
    "    2. Estabilidad de las gradientes.\n",
    "* Para entender estos dos problemos, recordemos que:\n",
    "    * Los RNNs estan calculando un _hidden state vector_ en cada time step usando el _hidden state vector_ del time step anterior y el _input vector_ en el time step actual.\n",
    "    * Este calculo hace al RNN bastante poderoso pero tambien crea problemas numericos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retener informacion a largo plazo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En cada time step estamos actualizando el _hidden state vector_ sin importar si hace sentido.\n",
    "* Como consecuencia, el RNN no tien control sobre cuales valores se retienen y cuales se descartan en el _hidden state_\n",
    "* Lo que deseamos es tener alguna forma de controlar:\n",
    "    * si la actualizacion es opcional\n",
    "    * o si deberia pasar,\n",
    "    * por cuanto?\n",
    "    * que partes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas de estabilidad numerica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El Elman RNN tiene una tiende a causar que las gradientes se salgan de control hacia cero o hacia infinito.\n",
    "* Hay dos tipos de gradientes inestables que se salen de control:\n",
    "    * _vanishing gradients_ cuando la direccion del valor absoluto de la gradiente se encoge\n",
    "    * _exploding gradients_ cuando la direccion del valor absoluto de la gradiente se expande\n",
    "* Cualquiera de estos dos problemas puede hacer el proceso de optimizacion inestable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gating como una solucion a los problemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para entender gating de forma intuitiva supongamos que estamos sumando dos cantidades, $a$ y $b$, pero queremos controlar cuanto de $b$ entra a la suma. Podemos reescribir la suma de $a + b$ como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ a + \\lambda b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde $\\lambda$ es un valor $[0, 1]$.\n",
    "* Si $\\lambda = 0$ entonces no hay ninguna contribucion de $b$\n",
    "* Si $\\lambda = 1$ entonces $b$ contribuye completamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ esta actuando como un _switch_ o un _gate_ que controla la cantidad de $b$ que entra a la suma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporando Gating al Elman RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el _hidden state_ previo era $h_{t-1}$ y el input actual es $x_t$, la actualizacion recurrente en el Elman RNN se veria algo asi:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h_t = h_{t-1} + F(h_{t-1}, x_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde,\n",
    "* $F$ es el calculo recurrente del RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ahora supongamos que, en vez de ser un constante, el $\\lambda$ en el ejemplo previo era una funcion de: \n",
    "    * _hidden state vector_ $h_{t-1}$ y \n",
    "    * el input actual $x_t$, y produjera el comportamiento deseado de _gating_.\n",
    "* Es decir, un valor $[0,1]$\n",
    "* Con esta funcion de _gating_, la ecuacion para la actualizacion del RNN seria la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h_t = h_{t-1} + \\lambda(h_{t-1}, x_t)F(h_{t-1}, x_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Podemos ver que la funcion $\\lambda$ controla cuanto de del input actual puede actualizar es estado $h_{t-1}$\n",
    "* La funcion $\\lambda$ es usualmente una funcion sigmoide que sabemos produce un valor entre $[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En el caso de una red _long short-term memory_ la intuicion basica se extiende para incorporar no solo actualizaciones condicionadas, pero tambien olvido intencional de los valores en el _hidden state_ previo $h_{t-1}$.\n",
    "* Esta funcion de _forgetting_ sucede multiplicando el _hidden state_ previo $h_{t-1}$ por otra funcion, $\\mu$, que tambien produce un valor entre $[0,1]$ y depende del input actual:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h_t = \\mu(h{t-1}, x_t)h_{t-1} + \\lambda(h_{t-1}, x_t)F(h_{t-1}, x_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\mu$ es otra funcion de _gating_\n",
    "* En una descripcion real del LSTM, se vuelve un poco mas complicado porque las _gating functions_ son parametrizadas, lo que lleva a una secuencia de operaciones un poco complejas.\n",
    "* Para conocer mas sobre LSTMs lean el [articulo clasico por Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El LSTM es una de las muchas variaciones con _gates_ del RNN. \n",
    "* Otra de las mas populares es el _gated recurrent unit_ GRU\n",
    "* En PyTorch simplemente se puede reemplazar el `nn.RNN` o `nn.RNNCell` con un `nn.LSTM` o `nn.LSTMCell`\n",
    "* Igual para un `nn.GRU`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
